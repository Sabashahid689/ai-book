"use strict";(globalThis.webpackChunkmy_book=globalThis.webpackChunkmy_book||[]).push([[348],{3023:(e,n,r)=>{r.d(n,{R:()=>s,x:()=>l});var i=r(3696);const o={},t=i.createContext(o);function s(e){const n=i.useContext(t);return i.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function l(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(o):e.components||o:s(e.components),i.createElement(t.Provider,{value:n},e.children)}},7072:(e,n,r)=>{r.r(n),r.d(n,{assets:()=>c,contentTitle:()=>l,default:()=>p,frontMatter:()=>s,metadata:()=>i,toc:()=>a});const i=JSON.parse('{"id":"chapter-6-capstone-ai-robot-pipeline","title":"Chapter 6 - Capstone: Simple AI-Robot Pipeline","description":"Learning Objectives","source":"@site/docs/chapter-6-capstone-ai-robot-pipeline.md","sourceDirName":".","slug":"/chapter-6-capstone-ai-robot-pipeline","permalink":"/docs/chapter-6-capstone-ai-robot-pipeline","draft":false,"unlisted":false,"editUrl":"https://github.com/Sabashahid689/ai-book/tree/main/docs/chapter-6-capstone-ai-robot-pipeline.md","tags":[],"version":"current","sidebarPosition":6,"frontMatter":{"id":"chapter-6-capstone-ai-robot-pipeline","title":"Chapter 6 - Capstone: Simple AI-Robot Pipeline","sidebar_label":"6. Capstone Project","sidebar_position":6},"sidebar":"textbookSidebar","previous":{"title":"5. Vision-Language-Action (VLA)","permalink":"/docs/chapter-5-vision-language-action-systems"}}');var o=r(2540),t=r(3023);const s={id:"chapter-6-capstone-ai-robot-pipeline",title:"Chapter 6 - Capstone: Simple AI-Robot Pipeline",sidebar_label:"6. Capstone Project",sidebar_position:6},l="Chapter 6: Capstone - Simple AI-Robot Pipeline",c={},a=[{value:"Learning Objectives",id:"learning-objectives",level:2},{value:"Introduction",id:"introduction",level:2},{value:"Core Concepts",id:"core-concepts",level:2},{value:"System Architecture",id:"system-architecture",level:3},{value:"Design Principles",id:"design-principles",level:3},{value:"Example: Object Pick-and-Place Pipeline",id:"example-object-pick-and-place-pipeline",level:3},{value:"Practical Application",id:"practical-application",level:2},{value:"Project: Autonomous Object Sorter",id:"project-autonomous-object-sorter",level:3},{value:"Step 1: Perception - Object Detection",id:"step-1-perception---object-detection",level:4},{value:"Step 2: Cognition - Decision Making",id:"step-2-cognition---decision-making",level:4},{value:"Step 3: Planning - Motion Planning",id:"step-3-planning---motion-planning",level:4},{value:"Step 4: Control - Execution",id:"step-4-control---execution",level:4},{value:"Integration: Launch File",id:"integration-launch-file",level:3},{value:"Testing the Pipeline",id:"testing-the-pipeline",level:3},{value:"Best Practices",id:"best-practices",level:2},{value:"1. Safety First",id:"1-safety-first",level:3},{value:"2. Robust Error Handling",id:"2-robust-error-handling",level:3},{value:"3. Modular Design",id:"3-modular-design",level:3},{value:"4. Testing Strategy",id:"4-testing-strategy",level:3},{value:"Summary",id:"summary",level:2},{value:"Further Reading",id:"further-reading",level:2}];function d(e){const n={a:"a",code:"code",h1:"h1",h2:"h2",h3:"h3",h4:"h4",header:"header",hr:"hr",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,t.R)(),...e.components};return(0,o.jsxs)(o.Fragment,{children:[(0,o.jsx)(n.header,{children:(0,o.jsx)(n.h1,{id:"chapter-6-capstone---simple-ai-robot-pipeline",children:"Chapter 6: Capstone - Simple AI-Robot Pipeline"})}),"\n",(0,o.jsx)(n.h2,{id:"learning-objectives",children:"Learning Objectives"}),"\n",(0,o.jsx)(n.p,{children:"By the end of this chapter, you will be able to:"}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsx)(n.li,{children:"Design an end-to-end AI-robot pipeline from perception to action"}),"\n",(0,o.jsx)(n.li,{children:"Integrate ROS 2, simulation, and AI models in a cohesive system"}),"\n",(0,o.jsx)(n.li,{children:"Apply best practices for robot software architecture"}),"\n",(0,o.jsx)(n.li,{children:"Deploy and test a complete robotic application"}),"\n"]}),"\n",(0,o.jsx)(n.h2,{id:"introduction",children:"Introduction"}),"\n",(0,o.jsxs)(n.p,{children:["This capstone chapter brings together concepts from all previous chapters to build a complete ",(0,o.jsx)(n.strong,{children:"AI-robot pipeline"}),". You'll create a system that:"]}),"\n",(0,o.jsxs)(n.ol,{children:["\n",(0,o.jsx)(n.li,{children:"Perceives the environment with sensors"}),"\n",(0,o.jsx)(n.li,{children:"Processes data with AI models"}),"\n",(0,o.jsx)(n.li,{children:"Plans and executes actions"}),"\n",(0,o.jsx)(n.li,{children:"Operates safely in simulation and on real hardware"}),"\n"]}),"\n",(0,o.jsx)(n.p,{children:"This hands-on project demonstrates the full software stack for Physical AI systems."}),"\n",(0,o.jsx)(n.h2,{id:"core-concepts",children:"Core Concepts"}),"\n",(0,o.jsx)(n.h3,{id:"system-architecture",children:"System Architecture"}),"\n",(0,o.jsx)(n.p,{children:"A typical AI-robot pipeline consists of four layers:"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{children:"\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\r\n\u2502  Perception Layer (Sensors \u2192 AI Models) \u2502\r\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\r\n\u2502  Cognition Layer (Decision Making)      \u2502\r\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\r\n\u2502  Planning Layer (Motion & Task Planning)\u2502\r\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\r\n\u2502  Control Layer (Actuator Commands)      \u2502\r\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n"})}),"\n",(0,o.jsx)(n.h3,{id:"design-principles",children:"Design Principles"}),"\n",(0,o.jsxs)(n.ol,{children:["\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Modularity"}),": Each component is an independent ROS 2 node"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Robustness"}),": Handle sensor failures and unexpected inputs"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Safety"}),": Emergency stop and collision avoidance"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Testability"}),": Validate each layer independently"]}),"\n"]}),"\n",(0,o.jsx)(n.h3,{id:"example-object-pick-and-place-pipeline",children:"Example: Object Pick-and-Place Pipeline"}),"\n",(0,o.jsxs)(n.p,{children:[(0,o.jsx)(n.strong,{children:"Task"}),": Detect an object, grasp it, and place it in a target location."]}),"\n",(0,o.jsxs)(n.p,{children:[(0,o.jsx)(n.strong,{children:"Pipeline"}),":"]}),"\n",(0,o.jsxs)(n.ol,{children:["\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Perception"}),": Camera detects object position"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Cognition"}),": Decide whether object is graspable"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Planning"}),": Compute arm trajectory to reach object"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Control"}),": Execute trajectory and close gripper"]}),"\n"]}),"\n",(0,o.jsx)(n.h2,{id:"practical-application",children:"Practical Application"}),"\n",(0,o.jsx)(n.h3,{id:"project-autonomous-object-sorter",children:"Project: Autonomous Object Sorter"}),"\n",(0,o.jsxs)(n.p,{children:[(0,o.jsx)(n.strong,{children:"Objective"}),": Build a robot that sorts colored blocks into bins."]}),"\n",(0,o.jsx)(n.h4,{id:"step-1-perception---object-detection",children:"Step 1: Perception - Object Detection"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-python",children:"import rclpy\r\nfrom rclpy.node import Node\r\nfrom sensor_msgs.msg import Image\r\nfrom cv_bridge import CvBridge\r\nimport cv2\r\nimport numpy as np\r\n\r\nclass ObjectDetector(Node):\r\n    def __init__(self):\r\n        super().__init__('object_detector')\r\n        self.subscription = self.create_subscription(\r\n            Image, '/camera/image', self.image_callback, 10)\r\n        self.publisher = self.create_publisher(\r\n            DetectedObjects, '/detected_objects', 10)\r\n        self.bridge = CvBridge()\r\n\r\n    def image_callback(self, msg):\r\n        # Convert ROS Image to OpenCV format\r\n        cv_image = self.bridge.imgmsg_to_cv2(msg, 'bgr8')\r\n\r\n        # Simple color-based detection (red blocks)\r\n        hsv = cv2.cvtColor(cv_image, cv2.COLOR_BGR2HSV)\r\n        lower_red = np.array([0, 100, 100])\r\n        upper_red = np.array([10, 255, 255])\r\n        mask = cv2.inRange(hsv, lower_red, upper_red)\r\n\r\n        # Find contours\r\n        contours, _ = cv2.findContours(mask, cv2.RETR_EXTERNAL,\r\n                                       cv2.CHAIN_APPROX_SIMPLE)\r\n\r\n        # Publish detected objects\r\n        objects = DetectedObjects()\r\n        for contour in contours:\r\n            x, y, w, h = cv2.boundingRect(contour)\r\n            objects.objects.append(Object(x=x, y=y, color='red'))\r\n\r\n        self.publisher.publish(objects)\n"})}),"\n",(0,o.jsx)(n.h4,{id:"step-2-cognition---decision-making",children:"Step 2: Cognition - Decision Making"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-python",children:"class TaskPlanner(Node):\r\n    def __init__(self):\r\n        super().__init__('task_planner')\r\n        self.subscription = self.create_subscription(\r\n            DetectedObjects, '/detected_objects', self.plan_callback, 10)\r\n        self.publisher = self.create_publisher(\r\n            TaskGoal, '/task_goal', 10)\r\n\r\n    def plan_callback(self, msg):\r\n        if len(msg.objects) == 0:\r\n            return  # No objects detected\r\n\r\n        # Pick closest object\r\n        closest_obj = min(msg.objects, key=lambda obj: obj.x**2 + obj.y**2)\r\n\r\n        # Determine target bin based on color\r\n        target_bin = self.get_bin_for_color(closest_obj.color)\r\n\r\n        # Publish task goal\r\n        goal = TaskGoal()\r\n        goal.action = 'pick_and_place'\r\n        goal.object_position = [closest_obj.x, closest_obj.y]\r\n        goal.target_position = target_bin\r\n        self.publisher.publish(goal)\r\n\r\n    def get_bin_for_color(self, color):\r\n        bins = {'red': [1.0, 0.0], 'blue': [-1.0, 0.0]}\r\n        return bins.get(color, [0.0, 0.0])\n"})}),"\n",(0,o.jsx)(n.h4,{id:"step-3-planning---motion-planning",children:"Step 3: Planning - Motion Planning"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-python",children:"from moveit_msgs.msg import MoveGroupActionGoal\r\n\r\nclass MotionPlanner(Node):\r\n    def __init__(self):\r\n        super().__init__('motion_planner')\r\n        self.subscription = self.create_subscription(\r\n            TaskGoal, '/task_goal', self.plan_motion, 10)\r\n        self.moveit_client = ActionClient(self, MoveGroup, 'move_group')\r\n\r\n    def plan_motion(self, goal):\r\n        # Convert object position to 3D coordinates\r\n        target_pose = self.pixel_to_world(goal.object_position)\r\n\r\n        # Create MoveIt goal\r\n        moveit_goal = MoveGroupActionGoal()\r\n        moveit_goal.request.group_name = 'manipulator'\r\n        moveit_goal.request.target_pose = target_pose\r\n\r\n        # Send goal to MoveIt\r\n        self.moveit_client.send_goal_async(moveit_goal)\r\n\r\n    def pixel_to_world(self, pixel_coords):\r\n        # Camera calibration: convert 2D pixel to 3D world coordinates\r\n        # Simplified example\r\n        x = pixel_coords[0] * 0.001  # Scale factor\r\n        y = pixel_coords[1] * 0.001\r\n        z = 0.5  # Fixed height above table\r\n        return [x, y, z]\n"})}),"\n",(0,o.jsx)(n.h4,{id:"step-4-control---execution",children:"Step 4: Control - Execution"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-python",children:"class RobotController(Node):\r\n    def __init__(self):\r\n        super().__init__('robot_controller')\r\n        self.joint_pub = self.create_publisher(\r\n            JointTrajectory, '/joint_trajectory', 10)\r\n        self.gripper_pub = self.create_publisher(\r\n            GripperCommand, '/gripper_command', 10)\r\n\r\n    def execute_trajectory(self, trajectory):\r\n        # Publish joint trajectory\r\n        self.joint_pub.publish(trajectory)\r\n\r\n    def close_gripper(self):\r\n        cmd = GripperCommand()\r\n        cmd.position = 0.0  # Fully closed\r\n        self.gripper_pub.publish(cmd)\r\n\r\n    def open_gripper(self):\r\n        cmd = GripperCommand()\r\n        cmd.position = 0.08  # Fully open\r\n        self.gripper_pub.publish(cmd)\n"})}),"\n",(0,o.jsx)(n.h3,{id:"integration-launch-file",children:"Integration: Launch File"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-python",children:"from launch import LaunchDescription\r\nfrom launch_ros.actions import Node\r\n\r\ndef generate_launch_description():\r\n    return LaunchDescription([\r\n        # Perception\r\n        Node(\r\n            package='my_robot',\r\n            executable='object_detector',\r\n            name='object_detector'\r\n        ),\r\n        # Cognition\r\n        Node(\r\n            package='my_robot',\r\n            executable='task_planner',\r\n            name='task_planner'\r\n        ),\r\n        # Planning\r\n        Node(\r\n            package='my_robot',\r\n            executable='motion_planner',\r\n            name='motion_planner'\r\n        ),\r\n        # Control\r\n        Node(\r\n            package='my_robot',\r\n            executable='robot_controller',\r\n            name='robot_controller'\r\n        ),\r\n        # Simulation\r\n        Node(\r\n            package='gazebo_ros',\r\n            executable='spawn_entity.py',\r\n            arguments=['-file', 'robot.urdf', '-entity', 'my_robot']\r\n        )\r\n    ])\n"})}),"\n",(0,o.jsx)(n.h3,{id:"testing-the-pipeline",children:"Testing the Pipeline"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-bash",children:"# 1. Launch Gazebo simulation\r\nros2 launch gazebo_ros gazebo.launch.py\r\n\r\n# 2. Launch the complete pipeline\r\nros2 launch my_robot object_sorter.launch.py\r\n\r\n# 3. Monitor system\r\nros2 node list\r\nros2 topic echo /detected_objects\r\nros2 topic echo /task_goal\r\n\r\n# 4. Visualize in RViz\r\nros2 run rviz2 rviz2\n"})}),"\n",(0,o.jsx)(n.h2,{id:"best-practices",children:"Best Practices"}),"\n",(0,o.jsx)(n.h3,{id:"1-safety-first",children:"1. Safety First"}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsxs)(n.li,{children:["Implement ",(0,o.jsx)(n.strong,{children:"emergency stop"}),": Hardware button to halt all motion"]}),"\n",(0,o.jsxs)(n.li,{children:["Add ",(0,o.jsx)(n.strong,{children:"collision detection"}),": Monitor force sensors and stop on contact"]}),"\n",(0,o.jsxs)(n.li,{children:["Use ",(0,o.jsx)(n.strong,{children:"velocity limits"}),": Cap maximum joint speeds"]}),"\n",(0,o.jsxs)(n.li,{children:["Test in ",(0,o.jsx)(n.strong,{children:"simulation"})," before deploying to hardware"]}),"\n"]}),"\n",(0,o.jsx)(n.h3,{id:"2-robust-error-handling",children:"2. Robust Error Handling"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-python",children:"def safe_execute(self, action):\r\n    try:\r\n        result = self.execute_action(action)\r\n        return result\r\n    except Exception as e:\r\n        self.get_logger().error(f'Execution failed: {e}')\r\n        self.emergency_stop()\r\n        return None\n"})}),"\n",(0,o.jsx)(n.h3,{id:"3-modular-design",children:"3. Modular Design"}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsx)(n.li,{children:"One node = one responsibility"}),"\n",(0,o.jsx)(n.li,{children:"Use well-defined message interfaces"}),"\n",(0,o.jsx)(n.li,{children:"Avoid tight coupling between components"}),"\n"]}),"\n",(0,o.jsx)(n.h3,{id:"4-testing-strategy",children:"4. Testing Strategy"}),"\n",(0,o.jsxs)(n.ol,{children:["\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Unit tests"}),": Test individual nodes in isolation"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Integration tests"}),": Verify communication between nodes"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"System tests"}),": Run full pipeline in simulation"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Hardware tests"}),": Deploy to real robot incrementally"]}),"\n"]}),"\n",(0,o.jsx)(n.h2,{id:"summary",children:"Summary"}),"\n",(0,o.jsx)(n.p,{children:"Building an AI-robot pipeline requires integrating perception, cognition, planning, and control into a cohesive system. ROS 2 provides the framework for modular, scalable robotics software."}),"\n",(0,o.jsx)(n.p,{children:"This capstone project demonstrates how concepts from all previous chapters\u2014Physical AI, mechanics, ROS 2, simulation, and VLA models\u2014come together to create intelligent, autonomous systems."}),"\n",(0,o.jsx)(n.p,{children:(0,o.jsx)(n.strong,{children:"Key Takeaways:"})}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsx)(n.li,{children:"AI-robot pipelines consist of perception, cognition, planning, and control layers"}),"\n",(0,o.jsx)(n.li,{children:"ROS 2 enables modular architecture with independent nodes"}),"\n",(0,o.jsx)(n.li,{children:"Safety and robustness are critical for real-world deployment"}),"\n",(0,o.jsx)(n.li,{children:"Test in simulation before deploying to hardware"}),"\n"]}),"\n",(0,o.jsx)(n.h2,{id:"further-reading",children:"Further Reading"}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsxs)(n.li,{children:["\n",(0,o.jsxs)(n.p,{children:[(0,o.jsx)(n.strong,{children:"Project Ideas"}),":"]}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsx)(n.li,{children:"Autonomous navigation with obstacle avoidance"}),"\n",(0,o.jsx)(n.li,{children:"Visual servoing for precise manipulation"}),"\n",(0,o.jsx)(n.li,{children:"Multi-robot coordination for warehouse tasks"}),"\n"]}),"\n"]}),"\n",(0,o.jsxs)(n.li,{children:["\n",(0,o.jsxs)(n.p,{children:[(0,o.jsx)(n.strong,{children:"Advanced Topics"}),":"]}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Behavior trees"})," for complex task planning"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Model Predictive Control"})," for optimal trajectory execution"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Multi-sensor fusion"})," with Kalman filters"]}),"\n"]}),"\n"]}),"\n",(0,o.jsxs)(n.li,{children:["\n",(0,o.jsxs)(n.p,{children:[(0,o.jsx)(n.strong,{children:"Open-Source Projects"}),":"]}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.a,{href:"https://moveit.ros.org/",children:"MoveIt 2"})," - Motion planning framework"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.a,{href:"https://navigation.ros.org/",children:"Navigation2"})," - Autonomous navigation"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.a,{href:"https://github.com/ros-planning/moveit2_tutorials",children:"Manipulation"})," - Pick-and-place tutorials"]}),"\n"]}),"\n"]}),"\n",(0,o.jsxs)(n.li,{children:["\n",(0,o.jsxs)(n.p,{children:[(0,o.jsx)(n.strong,{children:"Communities"}),":"]}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsx)(n.li,{children:(0,o.jsx)(n.a,{href:"https://discourse.ros.org/",children:"ROS Discourse"})}),"\n",(0,o.jsx)(n.li,{children:(0,o.jsx)(n.a,{href:"https://physicalintelligence.slack.com/",children:"Physical AI Slack Groups"})}),"\n",(0,o.jsx)(n.li,{children:(0,o.jsx)(n.a,{href:"https://robotics.stackexchange.com/",children:"Robotics Stack Exchange"})}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,o.jsx)(n.hr,{}),"\n",(0,o.jsxs)(n.p,{children:[(0,o.jsx)(n.strong,{children:"Congratulations!"})," You've completed the Physical AI & Humanoid Robotics essentials course. You now have the foundational knowledge to design, build, and deploy AI-powered robotic systems."]}),"\n",(0,o.jsx)(n.p,{children:(0,o.jsx)(n.strong,{children:"Next Steps:"})}),"\n",(0,o.jsxs)(n.ol,{children:["\n",(0,o.jsx)(n.li,{children:"Build your own project using the concepts learned"}),"\n",(0,o.jsx)(n.li,{children:"Contribute to open-source robotics projects"}),"\n",(0,o.jsx)(n.li,{children:"Explore advanced topics in reinforcement learning and control theory"}),"\n",(0,o.jsx)(n.li,{children:"Join the Physical AI community and share your work"}),"\n"]})]})}function p(e={}){const{wrapper:n}={...(0,t.R)(),...e.components};return n?(0,o.jsx)(n,{...e,children:(0,o.jsx)(d,{...e})}):d(e)}}}]);